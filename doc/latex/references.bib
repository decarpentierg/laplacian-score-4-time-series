
@inproceedings{he_laplacian_2005,
	title = {Laplacian {Score} for {Feature} {Selection}},
	volume = {18},
	url = {https://proceedings.neurips.cc/paper/2005/hash/b5b03f06271f8917685d14cea7c6c50a-Abstract.html},
	abstract = {In supervised learning scenarios, feature selection has been studied widely in the literature. Selecting features in unsupervised learning scenarios is a much harder problem, due to the absence of class labels that would guide the search for relevant information. And, almost all of previous unsupervised feature selection methods are "wrapper" techniques that require a learning algorithm to evaluate the candidate feature subsets. In this paper, we propose a "filter" method for feature selection which is independent of any learning algorithm. Our method can be performed in either supervised or unsupervised fashion. The proposed method is based on the observation that, in many real world classification problems, data from the same class are often close to each other. The importance of a feature is evaluated by its power of locality preserving, or, Laplacian Score. We compare our method with data variance (unsupervised) and Fisher score (supervised) on two data sets. Experimental results demonstrate the effectiveness and efficiency of our algorithm.},
	urldate = {2023-03-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {He, Xiaofei and Cai, Deng and Niyogi, Partha},
	year = {2005},
	file = {Full Text PDF:/Users/jeremie/Zotero/storage/FAVBTLCU/He et al. - 2005 - Laplacian Score for Feature Selection.pdf:application/pdf},
}

@misc{noauthor_mini-project_nodate,
	title = {Mini-{Project}},
	file = {output.pdf:/Users/jeremie/Zotero/storage/E68DJHWR/output.pdf:application/pdf},
}

@misc{li_scikit-feature_2023,
	title = {scikit-feature},
	copyright = {GPL-2.0},
	url = {https://github.com/jundongl/scikit-feature/blob/48cffad4e88ff4b9d2f1c7baffb314d1b3303792/skfeature/function/similarity_based/lap_score.py},
	abstract = {open-source feature selection repository in python},
	urldate = {2023-03-05},
	author = {Li, Jundong},
	month = mar,
	year = {2023},
	note = {original-date: 2015-11-06T16:34:33Z},
}

@book{scheffe_analysis_1999,
	title = {The {Analysis} of {Variance}},
	isbn = {978-0-471-34505-3},
	abstract = {Originally published in 1959, this classic volume has had a major impact on generations of statisticians. Newly issued in the Wiley Classics Series, the book examines the basic theory of analysis of variance by considering several different mathematical models. Part I looks at the theory of fixed-effects models with independent observations of equal variance, while Part II begins to explore the analysis of variance in the case of other models.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Scheffé, Henry},
	month = mar,
	year = {1999},
	note = {Google-Books-ID: z9yUEAAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@article{guillame-bert_classication_nodate,
	title = {Classiﬁcation of {Time} {Sequences} using {Graphs} of {Temporal} {Constraints}},
	abstract = {We introduce two algorithms that learn to classify Symbolic and Scalar Time Sequences (SSTS); an extension of multivariate time series. An SSTS is a set of events and a set of scalars. An event is deﬁned by a symbol and a time-stamp. A scalar is deﬁned by a symbol and a function mapping a number for each possible time stamp of the data. The proposed algorithms rely on temporal patterns called Graph of Temporal Constraints (GTC). A GTC is a directed graph in which vertices express occurrences of speciﬁc events, and edges express temporal constraints between occurrences of pairs of events. Additionally, each vertex of a GTC can be augmented with numeric constraints on scalar values. We allow GTCs to be cyclic and/or disconnected. The ﬁrst of the introduced algorithms extracts sets of co-dependent GTCs to be used in a voting mechanism. The second algorithm builds decision forest like representations where each node is a GTC. In both algorithms, extraction of GTCs and model building are interleaved. Both algorithms are closely related to each other and they exhibit complementary properties including complexity, performance, and interpretability. The main novelties of this work reside in direct building of the model and eﬃcient learning of GTC structures. We explain the proposed algorithms and evaluate their performance against a diverse collection of 59 benchmark data sets. In these experiments, our algorithms come across as highly competitive and in most cases closely match or outperform state-of-the-art alternatives in terms of the computational speed while dominating in terms of the accuracy of classiﬁcation of time sequences.},
	language = {en},
	author = {Guillame-Bert, Mathieu and Dubrawski, Artur},
	file = {Guillame-Bert and Dubrawski - Classiﬁcation of Time Sequences using Graphs of Te.pdf:/Users/jeremie/Zotero/storage/PJ3W3QU8/Guillame-Bert and Dubrawski - Classiﬁcation of Time Sequences using Graphs of Te.pdf:application/pdf},
}

@article{kohavi_wrappers_1997,
	series = {Relevance},
	title = {Wrappers for feature subset selection},
	volume = {97},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S000437029700043X},
	doi = {10.1016/S0004-3702(97)00043-X},
	abstract = {In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes.},
	language = {en},
	number = {1},
	urldate = {2023-03-25},
	journal = {Artificial Intelligence},
	author = {Kohavi, Ron and John, George H.},
	month = dec,
	year = {1997},
	keywords = {Classification, Feature selection, Filter, Wrapper},
	pages = {273--324},
	file = {ScienceDirect Full Text PDF:/Users/jeremie/Zotero/storage/BBY53C2D/Kohavi et John - 1997 - Wrappers for feature subset selection.pdf:application/pdf;ScienceDirect Snapshot:/Users/jeremie/Zotero/storage/BY7WWL4C/S000437029700043X.html:text/html},
}

@article{guyon_introduction_2003,
	title = {An introduction to variable and feature selection},
	volume = {3},
	issn = {1532-4435},
	abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
	number = {null},
	journal = {The Journal of Machine Learning Research},
	author = {Guyon, Isabelle and Elisseeff, André},
	month = mar,
	year = {2003},
	pages = {1157--1182},
	file = {Full Text PDF:/Users/jeremie/Zotero/storage/92QHYAC6/Guyon et Elisseeff - 2003 - An introduction to variable and feature selection.pdf:application/pdf},
}

@inproceedings{munson_feature_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On {Feature} {Selection}, {Bias}-{Variance}, and {Bagging}},
	isbn = {978-3-642-04174-7},
	doi = {10.1007/978-3-642-04174-7_10},
	abstract = {We examine the mechanism by which feature selection improves the accuracy of supervised learning. An empirical bias/variance analysis as feature selection progresses indicates that the most accurate feature set corresponds to the best bias-variance trade-off point for the learning algorithm. Often, this is not the point separating relevant from irrelevant features, but where increasing variance outweighs the gains from adding more (weakly) relevant features. In other words, feature selection can be viewed as a variance reduction method that trades off the benefits of decreased variance (from the reduction in dimensionality) with the harm of increased bias (from eliminating some of the relevant features). If a variance reduction method like bagging is used, more (weakly) relevant features can be exploited and the most accurate feature set is usually larger. In many cases, the best performance is obtained by using all available features.},
	language = {en},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer},
	author = {Munson, M. Arthur and Caruana, Rich},
	editor = {Buntine, Wray and Grobelnik, Marko and Mladenić, Dunja and Shawe-Taylor, John},
	year = {2009},
	keywords = {Corruption Level, Feature Selection, Feature Subset, Irrelevant Feature, Mean Square Error},
	pages = {144--159},
	file = {Full Text PDF:/Users/jeremie/Zotero/storage/SKLWVM86/Munson et Caruana - 2009 - On Feature Selection, Bias-Variance, and Bagging.pdf:application/pdf},
}

@article{freedman_statistics_2007,
	title = {Statistics (international student edition)},
	journal = {Pisani, R. Purves, 4th edn. WW Norton \& Company, New York},
	author = {Freedman, David and Pisani, Robert and Purves, Roger},
	year = {2007},
}

@misc{gu_generalized_2012,
	title = {Generalized {Fisher} {Score} for {Feature} {Selection}},
	url = {http://arxiv.org/abs/1202.3725},
	doi = {10.48550/arXiv.1202.3725},
	abstract = {Fisher score is one of the most widely used supervised feature selection methods. However, it selects each feature independently according to their scores under the Fisher criterion, which leads to a suboptimal subset of features. In this paper, we present a generalized Fisher score to jointly select features. It aims at finding an subset of features, which maximize the lower bound of traditional Fisher score. The resulting feature selection problem is a mixed integer programming, which can be reformulated as a quadratically constrained linear programming (QCLP). It is solved by cutting plane algorithm, in each iteration of which a multiple kernel learning problem is solved alternatively by multivariate ridge regression and projected gradient descent. Experiments on benchmark data sets indicate that the proposed method outperforms Fisher score as well as many other state-of-the-art feature selection methods.},
	urldate = {2023-03-25},
	publisher = {arXiv},
	author = {Gu, Quanquan and Li, Zhenhui and Han, Jiawei},
	month = feb,
	year = {2012},
	note = {arXiv:1202.3725 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/jeremie/Zotero/storage/PYYRVCES/Gu et al. - 2012 - Generalized Fisher Score for Feature Selection.pdf:application/pdf;arXiv.org Snapshot:/Users/jeremie/Zotero/storage/3N59BRQ8/1202.html:text/html},
}

@article{darling_kolmogorov-smirnov_1957,
	title = {The {Kolmogorov}-{Smirnov}, {Cramer}-von {Mises} {Tests}},
	volume = {28},
	issn = {00034851},
	url = {http://www.jstor.org/stable/2237048},
	number = {4},
	urldate = {2023-03-25},
	journal = {The Annals of Mathematical Statistics},
	author = {Darling, D. A.},
	year = {1957},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {823--838},
}

@misc{bagnall_time_nodate,
	title = {Time {Series} {Classification} {Website}},
	url = {https://timeseriesclassification.com/dataset.php},
	urldate = {2023-03-25},
	author = {Bagnall, Anthony and Keogh, Eamonn and Lines, Jason and Bostrom, Aaron and Large, James and Middlehurst, Matthew},
	file = {Time Series Classification Website:/Users/jeremie/Zotero/storage/QG6K3DSP/dataset.html:text/html},
}

@misc{bagnall_earthquakes_nodate,
	title = {Earthquakes dataset},
	url = {https://timeseriesclassification.com/description.php?Dataset=Earthquakes},
	urldate = {2023-03-25},
	author = {Bagnall, Anthony},
	file = {Time Series Classification Website:/Users/jeremie/Zotero/storage/FA7DIAP5/description.html:text/html},
}

@phdthesis{olszewski_generalized_2001,
	address = {USA},
	type = {phd},
	title = {Generalized feature extraction for structural pattern recognition in time-series data},
	abstract = {Pattern recognition encompasses two fundamental tasks: description and classification. Given an object to analyze, a pattern recognition system first generates a description of it (i.e., the pattern) and then classifies the object based on that description (i.e., the recognition). Two general approaches for implementing pattern recognition systems, statistical and structural, employ different techniques for description and classification. Statistical approaches to pattern recognition use decision-theoretic concepts to discriminate among objects belonging to different groups based upon their quantitative features. Structural approaches to pattern recognition use syntactic grammars to discriminate among objects belonging to different groups based upon the arrangement of their morphological (i.e., shape-based or structural) features. Hybrid approaches to pattern recognition combine aspects of both statistical and structural pattern recognition. Structural pattern recognition systems are difficult to apply to new domains because implementation of both the description and classification tasks requires domain knowledge. Knowledge acquisition techniques necessary to obtain domain knowledge from experts are tedious and often fail to produce a complete and accurate knowledge base. Consequently, applications of structural pattern recognition have been primarily restricted to domains in which the set of useful morphological features has been established in the literature (e.g., speech recognition and character recognition) and the syntactic grammars can be composed by hand (e.g., electrocardiogram diagnosis). To overcome this limitation, a domain-independent approach to structural pattern recognition is needed that is capable of extracting morphological features and performing classification without relying on domain knowledge. A hybrid system that employs a statistical classification technique to perform discrimination based on structural features is a natural solution. While a statistical classifier is inherently domain independent, the domain knowledge necessary to support the description task can be eliminated with a set of generally-useful morphological features. Such a set of morphological features is suggested as the foundation for the development of a suite of structure detectors to perform generalized feature extraction for structural pattern recognition in time-series data. The ability of the suite of structure detectors to generate features useful for structural pattern recognition is evaluated by comparing the classification accuracies achieved when using the structure detectors versus commonly-used statistical feature extractors. Two real-world databases with markedly different characteristics and established ground truth serve as sources of data for the evaluation. The classification accuracies achieved using the features extracted by the structure detectors were consistently as good as or better than the classification accuracies achieved when using the features generated by the statistical feature extractors, thus demonstrating that the suite of structure detectors effectively performs generalized feature extraction for structural pattern recognition in time-series data.},
	school = {Carnegie Mellon University},
	author = {Olszewski, Robert Thomas},
	year = {2001},
	note = {AAI3040489
ISBN-10: 0493538712},
}

@misc{olszewski_wafer_nodate,
	title = {Wafer dataset},
	url = {https://timeseriesclassification.com/description.php?Dataset=Wafer},
	urldate = {2023-03-25},
	author = {Olszewski, Robert Thomas},
	file = {Time Series Classification Website:/Users/jeremie/Zotero/storage/X8GX9A5S/description.html:text/html},
}

@misc{brown_wormtwoclass_nodate,
	title = {WormTwoClass dataset},
	url = {https://timeseriesclassification.com/description.php?Dataset=WormsTwoClass},
	urldate = {2023-03-25},
	author = {Brown, Andre and Bagnall, Anthony},
	file = {Time Series Classification Website:/Users/jeremie/Zotero/storage/23AQU38Q/description.html:text/html},
}

@article{barandas_tsfel_2020,
	title = {{TSFEL}: {Time} {Series} {Feature} {Extraction} {Library}},
	volume = {11},
	issn = {2352-7110},
	shorttitle = {{TSFEL}},
	url = {https://www.sciencedirect.com/science/article/pii/S2352711020300017},
	doi = {10.1016/j.softx.2020.100456},
	abstract = {Time series feature extraction is one of the preliminary steps of conventional machine learning pipelines. Quite often, this process ends being a time consuming and complex task as data scientists must consider a combination between a multitude of domain knowledge factors and coding implementation. We present in this paper a Python package entitled Time Series Feature Extraction Library (TSFEL), which computes over 60 different features extracted across temporal, statistical and spectral domains. User customisation is achieved using either an online interface or a conventional Python package for more flexibility and integration into real deployment scenarios. TSFEL is designed to support the process of fast exploratory data analysis and feature extraction on time series with computational cost evaluation.},
	language = {en},
	urldate = {2023-03-25},
	journal = {SoftwareX},
	author = {Barandas, Marília and Folgado, Duarte and Fernandes, Letícia and Santos, Sara and Abreu, Mariana and Bota, Patrícia and Liu, Hui and Schultz, Tanja and Gamboa, Hugo},
	month = jan,
	year = {2020},
	keywords = {Feature extraction, Machine learning, Python, Time series},
	pages = {100456},
	file = {ScienceDirect Full Text PDF:/Users/jeremie/Zotero/storage/8GA2B9GQ/Barandas et al. - 2020 - TSFEL Time Series Feature Extraction Library.pdf:application/pdf;ScienceDirect Snapshot:/Users/jeremie/Zotero/storage/JJGN33VX/S2352711020300017.html:text/html},
}

@article{giorgino_computing_2009,
	title = {Computing and {Visualizing} {Dynamic} {Time} {Warping} {Alignments} in \textit{{R}} : {The} \textbf{dtw} {Package}},
	volume = {31},
	issn = {1548-7660},
	shorttitle = {Computing and {Visualizing} {Dynamic} {Time} {Warping} {Alignments} in \textit{{R}}},
	url = {http://www.jstatsoft.org/v31/i07/},
	doi = {10.18637/jss.v031.i07},
	abstract = {This introduction to the R package dtw is a (slightly) modiﬁed version of Giorgino (2009), published in the Journal of Statistical Software. Dynamic time warping is a popular technique for comparing time series, providing both a distance measure that is insensitive to local compression and stretches and the warping which optimally deforms one of the two input series onto the other. A variety of algorithms and constraints have been discussed in the literature. The dtw package provides an uniﬁcation of them; it allows R users to compute time series alignments mixing freely a variety of continuity constraints, restriction windows, endpoints, local distance deﬁnitions, and so on. The package also provides functions for visualizing alignments and constraints using several classic diagram types.},
	language = {en},
	number = {7},
	urldate = {2023-03-25},
	journal = {Journal of Statistical Software},
	author = {Giorgino, Toni},
	year = {2009},
	file = {Giorgino - 2009 - Computing and Visualizing Dynamic Time Warping Ali.pdf:/Users/jeremie/Zotero/storage/3IBG2AXQ/Giorgino - 2009 - Computing and Visualizing Dynamic Time Warping Ali.pdf:application/pdf},
}